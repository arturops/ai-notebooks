{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165087e8-d3f0-4ef1-ae17-bd4fd66df9ff",
   "metadata": {},
   "source": [
    "# Local PDF Questions\n",
    "In this notebook we use `python 3.11.6` and `Ollama` to interact with PDFs from you local computer\n",
    "\n",
    "## Table of Content\n",
    "1. Setup\n",
    "2. Ingesting PDFs\n",
    "3. Embeddings\n",
    "4. Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21eb8be-8138-4d58-a4c1-413e69e926a7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743f8c88-b877-49db-bb5d-6055ded3a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: chromadb 1.0.9\n",
      "Uninstalling chromadb-1.0.9:\n",
      "  Successfully uninstalled chromadb-1.0.9\n",
      "Found existing installation: protobuf 5.29.4\n",
      "Uninstalling protobuf-5.29.4:\n",
      "  Successfully uninstalled protobuf-5.29.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y chromadb protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aeb8c4-b1b3-41cb-850f-2161fb8719bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install using requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "# Individual pip installs \n",
    "# !pip install tqdm \"unstructured[all-docs]>=0.16.12\" langchain chromadb \n",
    "# !pip install langchain_community\n",
    "# !pip install langchain-text-splitters langchain-ollama protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c02de6-afa7-40b9-97f1-588e89459cea",
   "metadata": {},
   "source": [
    "## Ingesting PDFs\n",
    "It is required to create on the same directory level a folder `pdfs` and in there store your desired `pdf` file to use. \n",
    "> Make sure to set the `pdf_name` variable below to the name of your pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ec968a-5cc4-428c-82b2-2262b67001bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e07897-e1c0-4219-a8ba-e654d9d2fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdfs/ReAct-AI-agents.pdf\n",
      "Uploaded 'pdfs/ReAct-AI-agents.pdf'.\n"
     ]
    }
   ],
   "source": [
    "pdf_name = \"ReAct-AI-agents.pdf\"\n",
    "pdf_path = os.path.join(\"pdfs\", pdf_name)\n",
    "print(pdf_path)\n",
    "\n",
    "if pdf_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=pdf_path)\n",
    "    data = loader.load()\n",
    "    print(f\"Uploaded '{pdf_path}'.\")\n",
    "else:\n",
    "    print(\"Upload a pdf in 'pdfs' folder. Create the folder if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be438336-7a09-4f20-8a0d-65a3b9a2f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview (output is too long, so only uncomment for curiosity)\n",
    "# data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee12d3-fcb0-46ae-ba20-50983d8e1c23",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "This section requires that you have [Ollama](https://ollama.com/) installed in your local computer and that you have pulled an embedding model of your choice. For this notebook, we chose [nomic-embed-text](https://ollama.com/library/nomic-embed-text) as it has large context length and outperforms OpenAI's `text-embedding-ada-002` and `text-embedding-3-small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3bd21d9-4374-4540-91d0-44dda4a8481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 970aa74c0a90: 100% ▕██████████████████▏ 274 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling ce4a164fc046: 100% ▕██████████████████▏   17 B                         \u001b[K\n",
      "pulling 31df23ea7daa: 100% ▕██████████████████▏  420 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# pull the model in case you do not have it yet\n",
    "!ollama serve\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc990e99-a138-4633-bb1d-bca6567d3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED               \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    Less than a second ago    \n",
      "mxbai-embed-large:335m     468836162de7    669 MB    4 hours ago               \n",
      "nomic-embed-text:v1.5      0a109f422b47    274 MB    4 hours ago               \n",
      "llama3.2:3b                a80c4f17acd5    2.0 GB    4 hours ago               \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4b2d51-2cf7-4355-a841-6c3b3ab7854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"nomic-embed-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb52843e-a2a0-4a40-9ea3-9f00467714f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a700af18-ca98-4ae8-893d-dbb92e52a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk pdf\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=150)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d24a3e0a-8a56-428d-b10a-0a979b0a3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize two chunks (long output cell, uncomment to visualize if curious)\n",
    "# chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de6ce79-4ccb-42f3-8720-5afaed9510f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chunks to vector database (chunks get embeded)\n",
    "vdb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    persist_directory=None, # to Chroma an epehemral client aka in memory\n",
    "    embedding=OllamaEmbeddings(model=EMBEDDING_MODEL),\n",
    "    collection_name=\"local_rag\", # collection created in chromadb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ae47d-a29e-43be-adf4-78b98b9cb8bd",
   "metadata": {},
   "source": [
    "## Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bbb901-5118-4b83-ba48-481ee28201d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8651733f-d42d-4968-8ea6-5506341448d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "LOCAL_MODEL = \"llama3.2:3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57b644a-d71c-49e6-8d55-f369022a0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=LOCAL_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc68087-35f3-41f0-9328-a7f5f80b881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356427c3-0e08-42d4-b0a9-1e07f85accb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vdb.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f217a091-50db-4abf-9c1c-ecbc74602f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: \n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "187ccda1-26ca-466a-bfff-a34dc1445540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chain to run all pipeline of steps\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38123c1a-d648-40a9-8aaa-b960be294eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is this about?\n"
     ]
    }
   ],
   "source": [
    "# call the whole pipeline of steps aka do RAG\n",
    "# uncomment this for getting an input to type your question\n",
    "response = chain.invoke(input(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c860ef3-b04f-41b9-bfc6-405d3143819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This appears to be an abstract or description of a research paper or conference presentation, likely related to artificial intelligence and reinforcement learning. The text mentions \"ReAct\" which seems to be a type of behavior correction algorithm for reinforcement learning. Specifically, it describes a scenario where ReAct fails to produce desirable reasoning traces and actions due to a \"hallucinating thought\", but is corrected by a human editor who manually edits the thoughts to improve the outcome.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcc51fb6-106c-4a87-a10f-f52ee2b5a913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are the 5 main takeaways from the document:\\n\\n1. **ReAct outperforms Act in finding relevant products**: In the Webshop task, ReAct is able to find products that satisfy all target attributes, while Act relies on search and filtering.\\n\\n2. **ReAct uses reasoning to solve tasks**: The example trajectories show that ReAct uses reasoning to solve tasks, such as thinking about the product's flavor name and features before taking actions.\\n\\n3. **ReAct's thought process is explicit**: In contrast to Act, which appears to have a more implicit thought process, ReAct's thoughts are explicitly stated in its trajectory, allowing for a clearer understanding of how it arrives at a solution.\\n\\n4. **ReAct struggles with certain tasks due to limitations in its thought process**: The example trajectories also show that ReAct gets stuck trying to put a knife on a countertop, highlighting potential limitations in its thought process.\\n\\n5. **ReAct's strengths lie in its ability to reason and think explicitly about the task requirements**: Overall, the document suggests that ReAct's strengths lie in its ability to reason about the task requirements and think explicitly about how to solve the problem, which enables it to outperform Act in certain tasks.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp2 = chain.invoke(\"What are the main 5 take aways from the document?\")\n",
    "resp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "752abddb-2539-43d0-91e1-741ba086c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the vector db \n",
    "# (aka clean memory because chroma was ephemeral)\n",
    "vdb.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
